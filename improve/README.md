# ChatGLM-6B 提升计划
ChatGLM-6B 自发布以来，因为其出色的中文对话能力受到了广大用户的喜爱，发布一个月内 Github 的 star 数超过 1.7万，Hugging Face Repo 的下载量超过 80 万。 同时有一批基于 ChatGLM-6B 的[优秀开源项目](https://github.com/THUDM/ChatGLM-6B)出现，在各个平台也引起了广泛讨论。

但是因为训练数据中包含的任务类型限制，ChatGLM-6B 在部分实际场景上的表现还不能让人满意。我们提供的[微调代码](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)能够由用户基于用户自己的数据训练模型。但是，因为神经网络的[灾难性遗忘](https://picture.iczhiku.com/weixin/message1587593113355.html)问题，微调后的模型往往会失去在通用领域的对话能力，或者因为训练数据较少而缺乏泛化能力，这一问题在我们的issue区已经引起了很多讨论。同时进行模型微调对于硬件资源和个人经验都有更高的要求，不适合所有用户进行。

为了帮助大家更好地解决实际场景中的问题，我们启动了 ChatGLM-6B 提升计划。针对您在实际场景中遇到的 ChatGLM-6B 表现不好的任务，您可以向我们提交您的训练数据。我们会定期（每2-4周）对数据的合法性、有用性、正确性进行筛选，将筛选通过的数据加入到我们的训练中，并**更新开源的模型参数**。请您确保提交的数据不包含任何个人信息、商业秘密或可能危害国家安全、侵害第三方知识产权的内容。您提交的数据将和通用域的对话数据一起进行训练，从而在提升特定任务性能的同时保持模型的通用能力，并且取得更好的泛化性能。当然，您的数据如被筛选通过，也将优先获得我们最新版本的 ChatGLM-6B 模型的体验资格，以及更好的模型服务。根据之前的经验，针对不需要特定领域知识的任务，100条左右的数据即可大幅提升模型在该任务上的性能。**除为前述目的外，您提供的数据无论是否筛选通过，除非获得您的许可或根据国家法律规定和监管要求外，我们不会将您提供的数据对外公开或用于其他目的**。我们不甚感激您对大模型的关注，您的支持和意见将为我们优化大模型提供不断向前的动力和帮助！

如果您希望参与这项计划，请填写[问卷](https://www.wjx.cn/vm/rAoGx9X.aspx#)并按照指示上传您的数据。提交的数据为 `jsonline` 格式，每行的内容为
```json lines
{"prompt": "请根据以下标签为商品编写一段广告\n类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤", "response": "宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。"}
```
其中 `prompt` 部分为模型的输入，`response` 部分为期望的模型输出。为了保证模型的性能，请在输入中尽可能详细地表述任务的类型和期望的输出格式。完整的数据文件请参考 [data_sample.jsonl](data_sample.jsonl)。 
